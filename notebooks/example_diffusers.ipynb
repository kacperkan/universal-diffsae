{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline, UNet2DConditionModel\n",
    "\n",
    "import src.hooked_model.scheduler\n",
    "from src.hooked_model.hooked_model import HookedDiffusionModel\n",
    "from src.hooked_model.utils import get_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sd-legacy/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the hooked model interface with model from diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DConditionModel.from_pretrained(\n",
    "    model_name,\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = src.hooked_model.scheduler.DDIMScheduler.from_config(pipe.scheduler.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A photo of an astronaut riding a horse on mars\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to provide:\n",
    "- denoiser model - either UNet or Transformer based, the assumption is that it should predict noise\n",
    "- scheduler - it has to have certain fields and implement scale_model_input() and step() methods\n",
    "- encode_prompt - function that encodes prompt into embeddings\n",
    "- get_timesteps - function that returns discrete timesteps for the diffusion process given the number of inference steps\n",
    "- vae - VAE model for latent space encoding/decoding, if latent space model is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_model = HookedDiffusionModel(\n",
    "    model=model,\n",
    "    scheduler=scheduler,\n",
    "    encode_prompt=pipe.encode_prompt,\n",
    "    get_timesteps=get_timesteps,\n",
    "    vae=pipe.vae,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = hooked_model(\n",
    "    prompt=prompt,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(1),\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler = diffusers.DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "out2 = pipe(\n",
    "    prompt=prompt,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(1),\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out and out2 should be the exact same images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to gather activations at specific positions in the model\n",
    "\n",
    "All you need to do is to provide a list of positions you want to cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, cache_dict = hooked_model.run_with_cache(\n",
    "    prompt=prompt,\n",
    "    num_images_per_prompt=1,\n",
    "    device=\"cuda\",\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=50,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(1),\n",
    "    positions_to_cache=[\"up_blocks.1.attentions.1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dict[\"output\"][\"up_blocks.1.attentions.1\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
